{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e89fe921",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\91991\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os,cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ba11643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.utils import plot_model, load_img, to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2217e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e8cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"Face-CK/\"\n",
    "labels=[]\n",
    "for i in os.listdir(data_dir):\n",
    "    labels.append(i)\n",
    "no_labels=len(labels)\n",
    "print(\"No of labels : \",no_labels)\n",
    "print(\"Labels : \",labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22a5c65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(981, 48, 48, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images=[]\n",
    "for i in os.listdir(data_dir):\n",
    "    imgs=os.listdir(data_dir+\"/\"+i)\n",
    "    for img in imgs:\n",
    "        input_img=cv2.imread(data_dir+\"/\"+i+\"/\"+img)\n",
    "        input_img=cv2.resize(input_img,(48,48))\n",
    "        images.append(input_img)\n",
    "\n",
    "img_array=np.array(images)\n",
    "# img_array=img_array.astype('float32')\n",
    "img_array=img_array/255.\n",
    "img_array.shape\n",
    "# img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2152d65",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "img=img_array.shape[0]\n",
    "faces=np.ones((img,),dtype='float32')\n",
    "\n",
    "faces[0:134]=0 #135(angry)\n",
    "faces[135:188]=1 #54(contempt)\n",
    "faces[189:365]=2 #177(disgust)\n",
    "faces[366:440]=3 #75(fear)\n",
    "faces[441:647]=4 #207(happy)\n",
    "faces[648:731]=5 #84(sadness)\n",
    "faces[732:980]=6 #249(surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=to_categorical(faces,7)\n",
    "x,y=shuffle(img_array,y,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df32045",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.15,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb5cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeed6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_images = datagen.flow(x_train, y_train, batch_size=7)\n",
    "\n",
    "# Example: Display a few augmented images\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(2, 3, figsize=(20, 7))\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        augmented_sample = augmented_images.next()\n",
    "        ax[i, j].imshow(augmented_sample[0][0])\n",
    "        ax[i, j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004cd2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "modal=Sequential()\n",
    "modal.add(Conv2D(32, kernel_size=(3,3), activation='relu',input_shape=(48,48, 3)))\n",
    "modal.add(MaxPool2D(2,2))\n",
    "modal.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "modal.add(MaxPool2D(2,2))\n",
    "\n",
    "modal.add(Flatten())\n",
    "modal.add(Dense(128,activation='relu'))\n",
    "modal.add(Dense(7,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207942dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "modal.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb04c26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualkeras.layered_view(modal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3807ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "modal.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1783c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(Callback):\n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        if logs.get('accuracy')>=0.90:\n",
    "            print(\"\\nReached More than 90% accuraccy\")\n",
    "            self.model.stop_training=True\n",
    "callback=myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90951d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=modal.fit(\n",
    "    datagen.flow(x_train,y_train,batch_size=7),\n",
    "    epochs=10000,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b521544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['accuracy'],color='red')\n",
    "plt.plot(hist.history['val_accuracy'],color='orange',linestyle=\"dashed\")\n",
    "plt.plot(hist.history['loss'],color='green')\n",
    "plt.plot(hist.history['val_loss'],color='blue',linestyle=\"dashed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bacacca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=modal.predict(x_test)\n",
    "y_pred=[np.argmax(probas) for probas in y_pred]\n",
    "y_test=[np.argmax(probas) for probas in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310bdee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp=confusion_matrix(y_test,y_pred)\n",
    "cmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f20386",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "import seaborn as sns\n",
    "sns.heatmap(cmp,xticklabels=labels,yticklabels=labels,annot=True,fmt='d')\n",
    "plt.title(\"Confusion Matrix\",color=\"darkred\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db83daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import load_img\n",
    "happy_train=os.path.join(data_dir,'disgust')\n",
    "file=os.listdir(happy_train)\n",
    "sample=random.choice(file)\n",
    "img=load_img(os.path.join(happy_train,sample),target_size=(48,48))\n",
    "x=image.img_to_array(img)\n",
    "x=np.expand_dims(x,axis=0)\n",
    "val=modal.predict(x)\n",
    "# ['anger', 'contempt', 'disgust', 'fear', 'happy', 'sadness', 'surprise']\n",
    "print(val)\n",
    "plt.imshow(img)\n",
    "if val[0][0] == 1:\n",
    "    print(\"Anger\")\n",
    "elif val[0][1]==1:\n",
    "    print(\"Contempt\")\n",
    "elif val[0][2]==1:\n",
    "    print(\"disgust\")\n",
    "elif val[0][3]==1:\n",
    "    print(\"fear\")\n",
    "elif val[0][4]==1:\n",
    "    print(\"happy\")\n",
    "elif val[0][5]==1:\n",
    "    print(\"sadness\")\n",
    "elif val[0][6]==1:\n",
    "    print(\"surprise\")\n",
    "else:\n",
    "    print(\"Couldn't identify\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481792f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modal.save(\"face_emotion.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae386b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the pre-trained emotion detection model\n",
    "model = load_model('./face_emotion.h5')\n",
    "\n",
    "# Load the haarcascades for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "image_path = \"./Images/sreekar.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "if image is not None:\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "    num_labels = 7\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi = image[y:y+h, x:x+w]\n",
    "\n",
    "        framing = cv2.resize(roi, (48, 48))\n",
    "        framing = np.expand_dims(framing, axis=0)\n",
    "\n",
    "        # Predict emotion\n",
    "        emotion_probabilities = model.predict(framing)\n",
    "        predicted_label_index = np.argmax(emotion_probabilities)\n",
    "        predicted_label = labels[predicted_label_index]\n",
    "\n",
    "        # Display emotion on the image\n",
    "        cv2.putText(image, f'Emotion: {predicted_label}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "    # Display the image with emotions\n",
    "    cv2.imshow('Facial Emotion Recognition', image)\n",
    "    plt.imshow(image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    print(\"Error to load the image/update the permissions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eaa713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "model = load_model('./face_emotion.h5')\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "cap = cv2.VideoCapture(0)\n",
    "num_labels = 7\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi = frame[y:y+h, x:x+w]\n",
    "        framing = cv2.resize(roi, (48, 48))\n",
    "        framing = np.expand_dims(framing, axis=0)\n",
    "\n",
    "        # Predict emotion\n",
    "        emotion_probabilities = model.predict(framing)\n",
    "        predicted_label_index = np.argmax(emotion_probabilities)\n",
    "        predicted_label = labels[predicted_label_index]\n",
    "\n",
    "        # Display emotion on the frame\n",
    "        cv2.putText(frame, f'Emotion: {predicted_label}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Facial Emotion Recognition', frame)\n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
